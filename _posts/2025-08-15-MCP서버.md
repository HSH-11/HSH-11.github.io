---
layout: post
title: MCP서버 With 커서 AI
description: 나만의 MCP서버 구현해보기
image: '/images/2025-08-15-MCP서버/3869c5fe36cf3d9f2d626ffcfe68c988669ebac822538817bbfd24cff73da081.png'
tags: [Dev,AI,BackEnd]
tags_color: '#5F9EA0'
toc: true
---

# 구현 동기

백엔드 개발을 하다 보면 대부분의 작업이 REST API 중심으로 이뤄집니다.
 CRUD 요청을 처리하고, 데이터베이스에 값을 저장하거나 불러오는 익숙한 패턴이죠. 하지만 실제 서비스 환경에서는 단순 요청–응답 구조만으로는 해결되지 않는 순간이 옵니다.

최근 AI 서비스들이 폭발적으로 늘어나면서, 백엔드 개발자인 저 역시 “AI를 어떻게 서비스에 녹일 수 있을까?”라는 고민을 자주 하게 됐습니다.
 기존의 REST API 방식만으로도 AI 모델을 호출할 수는 있지만, 문제는 **실시간성과 데이터 흐름**이었습니다.

- AI 모델이 생성한 결과를 **바로 사용자에게 스트리밍 형태로 전달**하고 싶을 때
- 다수의 클라이언트가 동시에 AI 응답을 **구독(Subscribe)** 하고 있어야 할 때
- 여러 AI 엔진과 **프로토콜 단위로 연결·전환**해야 할 때

이런 요구사항을 만족시키려면 단순한 API 서버만으로는 부족했습니다.
 그래서 선택한 것이 바로 **MCP 서버**입니다.

MCP 서버는 이벤트 기반 구조 덕분에 **실시간 AI 추론 결과 전달**, **대화형 인터페이스 유지**, **다중 프로토콜 브리징**에 강점이 있습니다.
 저는 이를 활용해, 예를 들어 AI 챗봇이 생성하는 답변을 한 문장씩 스트리밍하거나,
 여러 AI 모듈을 하나의 서버에서 제어하고 통합하는 구조를 구현해보고자 했습니다.

------

## MCP 개념 이해하기

### MCP가 무엇인가요?

{: .important}

LLM이 외부의 다양한 도구와 구조화된 방식으로 상호작용 할 수 있도록 설계된 프로토콜입니다.

MCP를 통해 LLM은 계산기, 파일 검색기, API 서버, 클라우드 서비스 등 다양한 외부 도구를 호출하고, 작업을 자동화하거나 결과를 받아올 수 있습니다.

### LLM은 왜 도구가 필요한가요?

LLM은 방대한 정보를 바탕으로 텍스트를 생성하는 데 매우 뛰어난 성능을 보이지만, 실제 업무에 적용하려면 한계에 부딪히게 됩니다. 예를 들어 '현재 서울의 날씨는?'이라는 질문에 대해 LLM은 과거의 데이터를 기반으로 추측은 할 수 있어도, 실시간 정보를 직접 조회할 수는 없습니다. 이러한 문제를 해결하기 위해 등장한 것이 바로 외부 **도구**입니다.

![image-20250816003544664](../images/2025-08-15-MCP서버/image-20250816003544664-1755272147101-3.png)

최근에는 이러한 도구를 연결하여 LLM이 실제 행동까지 수행하도록 하는 AI 에이전트 기술이 주목받고 있습니다. AI 에이전트는 사용자가 자연어로 의도를 전달하고, LLM은 적절한 도구를 골라 작업을 분해하고 실행하는 방식입니다. 그러나 도구를 연결하는 구조는 생각보다 복잡하며, 특히 다양한 모델을 조합하거나 여러 개의 도구를 사용할수록 유지보수가 어려워지기 때문이죠.

### 기존 방식의 문제점

기존에도 LLM과 외부 도구를 연결하려는 시도로는 OpenAI의 Function Calling, 앤트로픽 클로드의 tool_use 메시지 구조, 랭체인의 에이전트 기반 방식 등 다양한 접근이 존재했습니다.

![image-20250816010057132](../images/2025-08-16-MCP서버/image-20250816010057132-1755273660914-8.png)

하지만 이들 방식이 서로 호환되지 않아, 동일한 도구라도 모델별로 입력 형식과 호출 방식이 달라 재사용이 어렵습니다.
멀티 LLM 환경에서는 연동 코드가 중복되고, 도구가 많아질수록 프롬프트에 각 도구의 사용법을 일일이 명시해야 합니다. 또한 함수의 입력값이나 반환값이 바뀔 경우, 관련 코드를 모두 수정해야 하는 번거로움이 뒤따릅니다.

{: .q-left}

> Call_Weather_API.py

```python
import os
import json
import requests
from openai import OpenAI

# 환경 변수 또는 직접 키 입력
openai_api_key = "sk-" # OpenAI 키 입력
weather_api_key = "" # OpenWeather 키 입력

client = OpenAI(api_key=openai_api_key)

# Function 정의 (OpenAI에 알려줄 함수 명세)
functions = [
    {
        "name": "get_current_weather",
        "description": "현재 날씨를 조회합니다",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "도시 이름"},
            },
            "required": ["location"],
        },
    }
]

# 사용자 질문
messages = [
    {"role": "user", "content": "서울 날씨 어때?"}
]

# Step 1: GPT에게 함수 호출 요청 생성시킴
response = client.chat.completions.create(
    model="gpt-4",
    messages=messages,
    functions=functions,
    function_call="auto"
)
message = response.choices[0].message

if message.function_call:
    function_name = message.function_call.name
    arguments = json.loads(message.function_call.arguments)
    location = arguments["location"]

    # OpenWeatherMap은 영어 도시명을 더 잘 인식함 → 한글일 경우 영문 변환 권장
    # 간단한 매핑 예시
    korean_to_english = {
        "서울": "Seoul",
        "부산": "Busan",
        "대구": "Daegu",
        "인천": "Incheon",
        "광주": "Gwangju",
        "대전": "Daejeon",
        "울산": "Ulsan",
        "제주": "Jeju"
    }
    location_eng = korean_to_english.get(location, location)  # 기본은 그대로

    # Step 3: 실제 날씨 API 호출
    weather_url = "https://api.openweathermap.org/data/2.5/weather"
    params = {
        "q": location_eng,
        "appid": weather_api_key,
        "units": "metric",  # 섭씨 온도
        "lang": "kr"
    }

    weather_response = requests.get(weather_url, params=params)
    weather_data = weather_response.json()

    # Step 4: 실패 시 처리
    if "main" not in weather_data:
        print("❌ 날씨 정보를 가져오는 데 실패했습니다.")
        print("에러 메시지:", weather_data.get("message", "알 수 없는 오류"))
        print("전체 응답:", weather_data)
        exit()

    # Step 5: 결과 구성
    weather_result = {
        "location": location_eng,
        "temperature": weather_data["main"]["temp"],
        "condition": weather_data["weather"][0]["description"]
    }

    # Step 6: GPT에게 도구 실행 결과 전달
    messages.append(message)  # GPT가 만든 function_call 메시지
    messages.append({
        "role": "function",
        "name": function_name,
        "content": json.dumps(weather_result)
    })

    final_response = client.chat.completions.create(
        model="gpt-4",
        messages=messages
    )

    print("\n 최종 응답:")
    print(final_response.choices[0].message.content)

else:
    print("GPT가 함수 호출을 요청하지 않았습니다.")
```

Function Calling과 실제 API 호출 코드를 연동하면 실시간 날씨를 받아올 수 있지만, 이러한 연동을 도구마다 매번 새롭게 구성해야 하며, 모델마다 연결 방식이 다르기 때문에 재사용이 어렵습니다. 이러한 문제를 해결하고, LLM과 도구를 더 일관되고 유연하게 연결할 수 있도록 제안된 것이 바로 MCP입니다.

------

### MCP의 구조

- **도구 정의**

 각 도구는 독립된 MCP 서버로 구현되고, 도구의 기능은 JSON구조로 표현되며, 클라이언트가 이를 LLM에 전달해 도구 선택 및 호출에 활용합니다. 이 구조에는 도구의 이름, 수행하는 작업에 대한 설명, 입력 파라미터의 데이터 형식과 제약 조건, 반환값의 구조등이 포함됩니다.

- **도구 불러오기 및 LLM 연동**

클라이언트(커서, 클로드 데스크톱 등)는 MCP 서버로부터 도구의 JSON 명세를 자동으로 불러오며, 이를 LLM에게 전달합니다. 이 정보는 자동으로 프롬프트에 포함되어 별도의 수작업 없이 도구의 사용법을 이해하고 호출할 수 있도록 지원합니다. 덕분에 개발자는 프롬프트에 함수 설명을 일일이 삽입하거나, LLM이 도구를 학습하도록 반복 훈련시킬 필요가 없습니다.

- **다중 도구 연결 및 에이전트 협업**

MCP는 하나의 LLM이 여러 MCP 서버와 동시에 연결될 수 있도록 설계되어 있기 때문에 LLM은 복수의 도구를 병렬로 호출하거나, 앞선 도구의 출력값을 다음 도구의 입력으로 연계하여 복합 작업을 처리할 수 있습니다. 정리하면, MCP는 단순한 도구 호출을 넘어서, 에이전트들이 유기적으로 협업할 수 있는 구조를 가능하게 하는 핵심 기반 기술입니다.

------

### MCP가 가져올 변화

- 단일 모델에서 멀티 에이전트 구조로의 변환
- 도구를 사용자가 직접 호출하던 방식에서, AI가 사용자 요청을 해석해 적절한 도구를 자동으로 선택하고 실행하는 방식으로 전환
- 반복적으로 구현하던 도구를 재사용 가능한 구조로 개선
- LLM 중심의 설계에서 에이전트 중심의 아키텍처로 진화

결과적으로 AI 생태계는 MCP와 같은 표준화된 연결 프로토콜을 중심으로 한 멀티 에이전트 협업 체계로 진화할 것입니다.

------

## MCP 동작 방식 이해하기

### MCP 아키텍처

MCP는 기본적으로 서버-클라이언트 아키텍처를 갖습니다.

![image-20250817195842668](../images/2025-08-15-MCP서버/image-20250817195842668-1755428328725-3.png)

예를 들어 사용자가 문서를 요약해달라고 하면, 클라이언트는 해당 요청을 PDF 검색기나 코드 실행기와 같은 MCP 서버에 전달하고, 서버는 이를 처리한 뒤 결과를 다시 반환합니다. 이처럼 MCP는 클라이언트와 LLM이 생성한 요청(tool_use)에 응답하고 결과 메시지(tool_result)를 반환하는 서버 역할을 합니다.

MCP는 전통적인 서버-클라이언트 구조를 따르지만, 클라이언트 측을 두 개의 구성 요소로 나누어 이해할 수 있습니다. 사용자와 직접 상호작용하는 상위 계층은 'MCP 호스트'로, 클로드 데스크톱, 커서, 스미더리 등과 같은 LLM 기반 애플리케이션이 이 역할을 수행합니다. MCP 호스트는 사용자 입력을 받고, LLM의 응답을 출력하는 사용자 인터페이스 환경을 포함합니다.

|    역할    |                             설명                             | 예시                                                         |
| :--------: | :----------------------------------------------------------: | ------------------------------------------------------------ |
|   호스트   |    사용자가 직접 상호작용하는 AI 에플리케이션 인터페이스     | 클로드 데스크톱, 커서, 스미더리, 윈드서프 등                 |
| 클라이언트 | 사용자 질문을 분석한 뒤 MCP 서버에 적절한 요청을 보내고, 도구 실행 결과를 다시 받아 호스트에 전달하는 역할 | 커서 내부  LLM 호출 모듈, 클로드 데스크톱의 MCP 요청 처리기 등 |
|    서버    | 도구를 실제로 실행하는 MCP 서버. 이 서버에는 다양한 기능(API, 파일 처리, 검색 등)이 등록되어 있으며, 클라이언트의 요청에 따라 실행 | 날씨 API, PDF 요약 서버, 웹 검색 서버 등                     |

동일한 MCP 서버를 여러 호스트에서 공동으로 사용할 수 있다는 점이 핵심입니다. 하나의 MCP 서버를 만들면 다양한 애플리케이션에서 재사용할 수 있기 때문에 사용자 입장에서는 더 일관된 경험을 제공받고, 개발장 입장에서도 유지보수와 확장에 유리한 구조를 구성할 수 있습니다.

**각 도구를 역할별로 분리된 서버로 구성시 장점**

1. 독립 실행 : MCP 서버는 각각 독립된 프로세스로 실행되기 때문에 기능별로 나누면 장애 격리와 유지보수에 효과적입니다.
2. 확장성 : 기능의 특성에 따라 서버마다 다른 확장 전략을 적용할 수 있습니다.
3. 재사용성 : 한 번 구축한 MCP 서버를 클로드 데스크톱, 커서, 윈드서프 등 여러 클라이언트에서 공유해 사용할 수 있습니다.
4. 모듈화 : MSA처럼 각 도구를 전체 시스템과 분리해 독립적으로 개발, 배포, 운영할 수 있게 해줍니다.

------

### MCP 동작 원리

1. **사용자 입력** : 사용자가 자연어로 질문 또는 명령을 입력합니다.
2. **LLM이 입력을 분석** : LLM은 사용자의 입력을 해석하고 Context를 파악합니다.
3. **외부 정보 필요 여부를 판단** : LLM이 입력을 해석한 뒤, 자체 지식만으로 답변이 가능한지, 아니면 외부 정보를 참조해야 하는지를 판단합니다.
4. **MCP 형식에 맞춘 요청 생성** : 외부 도구 호출이 필요한 경우, MCP 형식(JSON 요청)으로 변환하여 요청을 준비합니다.
5. **MCP 서버가 요청을 수신 및 해석** : MCP 서버는 클라이언트를 통해 전달된 요청을 수신하고, 요청된 도구를 실행. MCP 서버는 도구 선택에 관여하지 않으며, 요청에 포함된 정보에 따라 지정된 도구만을 실행합니다.
6. **외부 리소스 또는 도구를 호출** : MCP 서버가 요청된 도구(@mcp.tool로 등록된 함수 등)를 실행합니다. 도구에 따라 외부 API를 호출하거나, 데이터베이스 조회, 코드 실행 등의 작업을 수행하여 필요한 정보를 가져올 수 있습니다.
7. **데이터 수신 및 결과 정리** : MCP 서버는 도구 실행 결과를 JSON 형식으로 반환되며, 이 결과는 클라이언트를 통해 LLM에 전달되며, LLM은 이를 기반으로 후속 응답을 생성합니다.
8. **MCP 서버가 모델에 응답 전달** : 도구 실행 결과는 클라이언트를 통해 LLM에게 전달되며, 이후 LLM은 기존 대화 문맥과 결합해 응답을 생성합니다.
9. **AI 모델이 응답을 통합 및 재구성** : LLM은 MCP 서버에서 전달받은 응답을 기존 문맥과 통합하여 자연스럽게 정리합니다.
10. **사용자에게 최종 응답 출력** : LLM이 재구성한 응답을 사용자에게 출력하는 최종 단계입니다.

------

